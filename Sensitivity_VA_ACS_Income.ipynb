{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e8109a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = \"ignore\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from shap import GPUTreeExplainer\n",
    "from matplotlib.ticker import MaxNLocator,MultipleLocator\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.special import expit\n",
    "\n",
    "import mlresearch\n",
    "mlresearch.utils.set_matplotlib_style()\n",
    "from mlresearch.utils import set_matplotlib_style\n",
    "set_matplotlib_style(font_size=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b36a1592-b0c5-4100-b424-b7db710762cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n",
      "1.6.1\n",
      "2.1.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__) #1.26.4\n",
    "# print(shap.__version__) #0.46.1.dev86\n",
    "print(sklearn.__version__) #1.6.0\n",
    "print(xgb.__version__) #1.7.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f97860-e454-41e5-b150-445368cff597",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10cd23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use White alone & African American only \n",
    "FEAT_CNT = 8\n",
    "STATE = 'VA'\n",
    "FOLDS = 5\n",
    "seeds = [0,21,42,63,84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21fb6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols =['Occupation', 'Marriage','Place of Birth','Sex', 'Race']\n",
    "\n",
    "with open(file=f'dataset/ACS_Income_{STATE}.pickle', mode='rb') as f:\n",
    "    df=pickle.load(f)\n",
    "df = df[(df['Race']=='White alone') | (df['Race']== 'Black or African American alone')]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "columns = df.columns\n",
    "with pd.option_context('future.no_silent_downcasting', True):\n",
    "    df.replace([' <=50K',' >50K'],\n",
    "                 [0,1], inplace = True)\n",
    "    df['Sex'].replace( {'Female':0.0},inplace = True)\n",
    "    df['Sex'].replace({'Male':1.0}, inplace = True)\n",
    "X = df.iloc[:, 0:FEAT_CNT]\n",
    "Y = df.iloc[:, FEAT_CNT]\n",
    "\n",
    "category_col =['Occupation', 'Marriage','Place of Birth', 'Race']\n",
    "X = pd.get_dummies(X, columns=category_col, drop_first=True)\n",
    "for c in X.columns:\n",
    "    X[c] = X[c].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5fb1a-7d9d-4f4a-a71d-a65f066d472b",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6b12389d-a0f6-4dbc-b1f8-5b61b6a5dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(strategy, bin_size, seed):\n",
    "    ## compute bin boundary\n",
    "    np.random.seed(seed)\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "    for train_val_idx, test_idx in splitter.split(X, Y):\n",
    "        X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "        Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "        X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "        Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx] \n",
    "    kd = KBinsDiscretizer(n_bins=bin_size, encode='ordinal', strategy=strategy)\n",
    "\n",
    "    kd.fit(X_train)\n",
    "    bin_boundaries = kd.bin_edges_[0]\n",
    "\n",
    "    return bin_boundaries\n",
    "def assign_age(age,bin_edges):\n",
    "    # Assign age to a median of the bin_edges\n",
    "    for idx in range(len(bin_edges)-1):\n",
    "        if age == bin_edges[-1]:\n",
    "            median = (bin_edges[-1] + bin_edges[-2])/2\n",
    "        elif bin_edges[idx] <= age and age < bin_edges[idx+1]:\n",
    "            median = (bin_edges[idx] + bin_edges[idx+1])/2\n",
    "    return median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "71cbe3c8-5433-44e2-9ad6-a17618a9ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap(X_train,X_test,Y_train,Y_test, seed):\n",
    "\n",
    "    print('**********START**********')\n",
    "    # Train model on new data\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "        'classifier__max_depth': [3, 5, 7,9,11],          # Maximum tree depth\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage \n",
    "        'classifier__colsample_bytree': [0.8, 1.0],  # Subsample ratio of columns for each tree\n",
    "        'classifier__gamma': [0, 0.1, 0.2],          # Minimum loss reduction for a split\n",
    "    }\n",
    "    model = xgb.XGBClassifier(random_state=seed)\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid,              # 3-fold cross-validation\n",
    "        scoring='f1',   # Evaluation metric\n",
    "        n_jobs=-1,            # Use all processors\n",
    "        verbose=1             # Print progress\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "        \n",
    "    # Extract the best model\n",
    "    model = grid_search.best_estimator_\n",
    "\n",
    "    explainer = GPUTreeExplainer(model,X_train,feature_perturbation = 'interventional')\n",
    "    shap_values = explainer(X_test)\n",
    "    pred = best_model.predict(X_test)\n",
    "    return shap_values, pred\n",
    "    \n",
    "def get_tfs(shap_vals,Y_true, pred):\n",
    "    # Compute indices of errors\n",
    "    TP_i = np.where((Y_true == 1.0) & (pred == 1.0))[0]  # True Positives\n",
    "    FP_i = np.where((Y_true == 0.0) & (pred == 1.0))[0]  # False Positives\n",
    "    TN_i = np.where((Y_true == 0.0) & (pred == 0.0))[0]  # True Negatives\n",
    "    FN_i = np.where((Y_true == 1.0) & (pred == 0.0))[0]  # False Negatives\n",
    "    return TP_i,FP_i,TN_i,FN_i\n",
    "\n",
    "def get_ranks(shap_vals):\n",
    "\n",
    "    # avg_shap = np.mean(np.abs(shap_vals), axis=0)\n",
    "    # Compute rankings row-wise\n",
    "    sorted_indices = np.argsort(-np.abs(shap_vals), axis=1)  # Indices of absolute values in descending order\n",
    "    rank = np.empty_like(sorted_indices)             # Create an empty array of the same shape\n",
    "    rows, cols = shap_vals.shape                            # Get the shape of the array\n",
    "    rank[np.arange(rows)[:, None], sorted_indices] = np.arange(1, cols + 1)  # Assign ranks row-wise\n",
    "\n",
    "    target_rank = rank[:,0]\n",
    "\n",
    "    return rank,target_rank\n",
    "\n",
    "def compute_fidelity(pred, sv, base):\n",
    "\n",
    "    sv_sums = expit(np.sum(sv, axis=1)+base)\n",
    "    binary_predictions = (sv_sums > 0.5).astype(float)\n",
    "    fidelity = np.mean(binary_predictions == pred)\n",
    "    match_idx = np.where(binary_predictions == pred)[0]\n",
    "        \n",
    "    return fidelity,match_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e55d4",
   "metadata": {},
   "source": [
    "## Train the model with plain Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee15125f-6596-4bd0-ba71-c8a45d2efd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████▍                                     | 1/5 [00:15<01:03, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████▊                            | 2/5 [00:26<00:38, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████▏                  | 3/5 [00:37<00:23, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████▌         | 4/5 [00:47<00:11, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 5/5 [00:57<00:00, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall average acc: 79.50 average f1s : 76.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# base_models = list()\n",
    "base_shap_vals = list()\n",
    "base_preds = list()\n",
    "base_accs = list() \n",
    "base_f1s = list()\n",
    "base_ranks = list()\n",
    "base_age_ranks = list()\n",
    "\n",
    "base_tp_idx = list()\n",
    "base_fp_idx = list()\n",
    "base_tn_idx = list()\n",
    "base_fn_idx = list()\n",
    "\n",
    "base_tp_age_ranks = list()\n",
    "base_fp_age_ranks = list()\n",
    "base_tn_age_ranks = list()\n",
    "base_fn_age_ranks = list()\n",
    "\n",
    "base_firsts = list()\n",
    "base_percentages = list()\n",
    "for seed in tqdm(seeds):\n",
    "    np.random.seed(seed)\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "    for train_val_idx, test_idx in splitter.split(X, Y):\n",
    "        X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "        Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "        X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "        Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx]\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "        'classifier__max_depth': [3, 5, 7,9,11],          # Maximum tree depth\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage \n",
    "        'classifier__colsample_bytree': [0.8, 1.0],  # Subsample ratio of columns for each tree\n",
    "        'classifier__gamma': [0, 0.1, 0.2],          # Minimum loss reduction for a split\n",
    "    }\n",
    "    model = xgb.XGBClassifier(random_state=seed)\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid,              # 3-fold cross-validation\n",
    "        scoring='f1',   # Evaluation metric\n",
    "        n_jobs=-1,            # Use all processors\n",
    "        verbose=1             # Print progress\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    explainer = GPUTreeExplainer(best_model,X_train, feature_perturbation='interventional') \n",
    "    shap_values = explainer(X_test)\n",
    "    \n",
    "    sv = shap_values.values\n",
    "    base_rank,age_rank= get_ranks(sv)\n",
    "    base_ranks.append(base_rank)\n",
    "    \n",
    "    pred = best_model.predict(X_test)\n",
    "    # base_models.append(best_model)\n",
    "    base_shap_vals.append(shap_values)\n",
    "    base_preds.append(pred)\n",
    "    base_accs.append(accuracy_score(Y_test,pred)*100)\n",
    "    base_f1s.append(f1_score(Y_test,pred)*100)\n",
    "    base_ranks.append(base_rank)\n",
    "    base_age_ranks.append(age_rank)\n",
    "\n",
    "    # Errors\n",
    "    tp,fp,tn,fn = get_tfs(sv, Y_test,pred)\n",
    "    tp_rank, tp_age_rank = get_ranks(sv[tp])\n",
    "    fp_rank, fp_age_rank = get_ranks(sv[fp])\n",
    "    tn_rank, tn_age_rank = get_ranks(sv[tn])\n",
    "    fn_rank, fn_age_rank = get_ranks(sv[fn])\n",
    "    base_tp_idx.append(tp)\n",
    "    base_fp_idx.append(fp)\n",
    "    base_tn_idx.append(tn)\n",
    "    base_fn_idx.append(fn)\n",
    "\n",
    "    base_tp_age_ranks.append(tp_age_rank)\n",
    "    base_fp_age_ranks.append(fp_age_rank)\n",
    "    base_tn_age_ranks.append(tn_age_rank)\n",
    "    base_fn_age_ranks.append(fn_age_rank)\n",
    "    \n",
    "    # First\n",
    "\n",
    "    ## Indices of first\n",
    "    first = [int(j) for j,v in enumerate(age_rank) if v == 1]\n",
    "    base_firsts.append(first)\n",
    "    base_percentages.append(len(first)/len(X_test) )\n",
    "\n",
    "print(f'Overall average acc: {sum(base_accs)/len(base_accs):.2f} average f1s : {sum(base_f1s)/len(base_f1s):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e801569-54a8-4e56-b030-769fb5b5debf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2407"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e3c6b25-6330-4d34-ba30-f94bbd59b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "path = './results'\n",
    "if not os.path.exists(path):\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(path)\n",
    "\n",
    "# # save\n",
    "with open(path + '/Sens_Income_base_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_accs_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_accs, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_f1s_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_f1s, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open(path + '/Sens_Income_base_tp_idx_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_tp_idx, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_fp_idx_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_fp_idx, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_tn_idx_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_tn_idx, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_fn_idx_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_fn_idx, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "with open(path + '/Sens_Income_base_tp_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_tp_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_fp_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_fp_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_tn_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_tn_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_fn_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_fn_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path + '/Sens_Income_base_firsts_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_firsts, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_base_percentages_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_percentages, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99999868-c1a5-438c-b515-81e658202610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_first_rank_shift(base_ranks, rank_difs):\n",
    "#     first_rank_difs = list()\n",
    "#     first_indices = list()\n",
    "#     for idx,base_rank in enumerate(base_ranks):\n",
    "#         age_rank = base_rank[:,0]\n",
    "#         first = [int(j) for j in range(len(age_rank)) if int(age_rank[j]) == 1]\n",
    "#         first_rank_dif = [int(rank_difs[idx][k]) for k in first]\n",
    "#         first_rank_difs.append(first_rank_dif)\n",
    "#         first_indices.append(first)\n",
    "#     return first_rank_difs, first_indices\n",
    "\n",
    "\n",
    "\n",
    "# def compute_average_counts(l,decrease, etc=False):\n",
    "#     if not etc:\n",
    "#         s = sum([1 for r in l if r ==decrease])\n",
    "#     else:\n",
    "#         s = sum([1 for r in l if r <= decrease])\n",
    "#     percent = s/len(l)\n",
    "#     return percent\n",
    "    \n",
    "# def down_percent(rank_shifts):\n",
    "#     k_downs = list()\n",
    "#     for shifts in rank_shifts:\n",
    "#         downs = list()\n",
    "#         for decrease in range(-1,-5,-1):\n",
    "#             down_percent = compute_average_counts(shifts,decrease, decrease == -4)\n",
    "#             downs.append(down_percent)\n",
    "#         k_downs.append(downs)\n",
    "#         # print(f'number of shifts {len(shifts)}')\n",
    "    \n",
    "#     return np.mean(k_downs,axis = 0),np.std(k_downs,axis=0)\n",
    "\n",
    "# def get_bucket_idx(age, bin_edges):\n",
    "#     for idx in range(len(bin_edges)-1):\n",
    "#         if age == bin_edges[-1]:\n",
    "#             bin_idx = len(bin_edges) - 1\n",
    "#         elif bin_edges[idx] <= age and age < bin_edges[idx+1]:\n",
    "#             bin_idx = idx\n",
    "#             break\n",
    "#     return bin_idx\n",
    "        \n",
    "\n",
    "# def get_all_bucket_idx(X, edges):\n",
    "#     X2 = X.copy()\n",
    "#     X2['Age'] = X2['Age'].apply(lambda age: assign_age(age, edges)) \n",
    " \n",
    "#     np.random.seed(0)\n",
    "#     _, X_test, _, _ = train_test_split(X2, Y, test_size=0.3, random_state=0)\n",
    "#     bucket_indices = list()\n",
    "#     for age in X_test['Age']:\n",
    "#         cur_bucket_idx = get_bucket_idx(age, edges)\n",
    "#         bucket_indices.append(cur_bucket_idx)\n",
    "#     return bucket_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239881f2-fd00-4d5e-9f53-0c81d8158834",
   "metadata": {},
   "source": [
    "# Equi Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5dfcd38d-7e89-4b36-88f8-101ed6b886a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    }
   ],
   "source": [
    "ed_fids = list()\n",
    "ed_preds = list()\n",
    "ed_ranks = list()\n",
    "ed_age_ranks = list()\n",
    "ed_shap_vals = list()\n",
    "ed_rank_difs = list()\n",
    "\n",
    "ed_tp_age_ranks = list()\n",
    "ed_fp_age_ranks = list()\n",
    "ed_tn_age_ranks = list()\n",
    "ed_fn_age_ranks = list()\n",
    "\n",
    "ed_firsts = list()\n",
    "ed_percentages = list()\n",
    "ed_first_rank_difs = list()\n",
    "\n",
    "for bucket in range(2,21):\n",
    "    b_fids = list()\n",
    "    b_preds = list()\n",
    "    b_ranks = list()\n",
    "    b_age_ranks = list()\n",
    "    b_shap_vals = list()\n",
    "    b_rank_difs = list()\n",
    "\n",
    "    b_tp_age_ranks = list()\n",
    "    b_fp_age_ranks = list()\n",
    "    b_tn_age_ranks = list()\n",
    "    b_fn_age_ranks = list()\n",
    "\n",
    "    b_firsts = list()\n",
    "    b_percentages = list()\n",
    "    b_first_rank_difs = list()\n",
    "    for i, seed in enumerate(seeds):\n",
    "        \n",
    "        X2 = X.copy()\n",
    "        bucket_edge = get_bins('quantile',bucket,seed)\n",
    "        X2['Age'] = X2['Age'].apply(lambda age: assign_age(age, bucket_edge)) \n",
    "        np.random.seed(seed)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "        for train_val_idx, test_idx in splitter.split(X2, Y):\n",
    "            X_train_val, X_test = X2.iloc[train_val_idx], X2.iloc[test_idx]\n",
    "            Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "        for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "            X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "            Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx] \n",
    "\n",
    "        \n",
    "        # compute bin edges as shap values for bucketized data\n",
    "        \n",
    "        bucket_shap,bucket_pred = compute_shap(X_train,X_test,Y_train,Y_test,seed)\n",
    "        sv = bucket_shap.values\n",
    "        ed_rank, ed_age_rank = get_ranks(sv)\n",
    "        # Compute fidelity and indices where explanation is the same.\n",
    "\n",
    "        preds = base_preds[i]\n",
    "        sv = bucket_shap.values\n",
    "        base = bucket_shap.base_values\n",
    "        ed_fid, ed_agreed = compute_fidelity(preds,sv,base)\n",
    "\n",
    "        b_fids.append(ed_fid)\n",
    "        b_preds.append(bucket_pred)\n",
    "        b_ranks.append(ed_rank)\n",
    "        b_age_ranks.append(ed_age_rank)\n",
    "        b_shap_vals.append(bucket_shap)\n",
    "\n",
    "        \n",
    "        # Compuute rank shift\n",
    "        base_age_rank = base_age_ranks[i]\n",
    "        rank_dif = [r1-r2 for r1,r2 in zip(base_age_rank,ed_age_rank)]\n",
    "        b_rank_difs.append(rank_dif)\n",
    "\n",
    "        # Errors\n",
    "        tp,fp,tn,fn = get_tfs(sv, Y_test,pred)\n",
    "        tp_rank, tp_age_rank = get_ranks(sv[tp])\n",
    "        fp_rank, fp_age_rank = get_ranks(sv[fp])\n",
    "        tn_rank, tn_age_rank = get_ranks(sv[tn])\n",
    "        fn_rank, fn_age_rank = get_ranks(sv[fn])\n",
    "        b_tp_age_ranks.append(tp_rank)\n",
    "        b_fp_age_ranks.append(fp_rank)\n",
    "        b_tn_age_ranks.append(tn_rank)\n",
    "        b_fn_age_ranks.append(fn_rank)\n",
    "        \n",
    "        # First\n",
    "    \n",
    "        ## Indices of first\n",
    "        first = [int(j) for j,v in enumerate(ed_age_rank) if v == 1]\n",
    "        first_rank_dif = [rank_dif[idx] for idx in first]\n",
    "        \n",
    "        b_firsts.append(first)\n",
    "        b_percentages.append(len(first)/len(X_test)*100 )\n",
    "        b_first_rank_difs.append(first_rank_dif)\n",
    "        \n",
    "    ed_fids.append(b_fids)\n",
    "    ed_preds.append(b_preds)\n",
    "    ed_ranks.append(b_ranks)\n",
    "    ed_age_ranks.append(b_age_ranks)\n",
    "    ed_shap_vals.append(b_shap_vals)\n",
    "    ed_rank_difs.append(b_rank_difs)\n",
    "\n",
    "    ed_tp_age_ranks.append(b_tp_age_ranks)\n",
    "    ed_fp_age_ranks.append(b_fp_age_ranks)\n",
    "    ed_tn_age_ranks.append(b_tn_age_ranks)\n",
    "    ed_fn_age_ranks.append(b_fn_age_ranks)\n",
    "\n",
    "    ed_firsts.append(b_firsts)\n",
    "    ed_percentages.append(b_percentages)\n",
    "    ed_first_rank_difs.append(b_first_rank_difs)\n",
    "    # print(f'Average fidelity {sum(cut5_fidelities)/len(cut5_fidelities)*100:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaec20ea-6d9c-45b8-9dfe-f203cc2b4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "with open(path + '/Sens_Income_ed_fids_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_fids, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_rank_difs_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_rank_difs, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path + '/Sens_Income_ed_tp_ranks_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_tp_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_fp_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_fp_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_tn_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_tn_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_fn_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_fn_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path + '/Sens_Income_ed_firsts_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_firsts, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_percentages_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_percentages, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ed_first_rank_difss_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ed_first_rank_difs, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c3611",
   "metadata": {},
   "source": [
    " # Equi Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "050bf71c-5fab-41fa-9844-8e9ee18b07a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    }
   ],
   "source": [
    "ew_fids = list()\n",
    "ew_preds = list()\n",
    "ew_ranks = list()\n",
    "ew_age_ranks = list()\n",
    "ew_shap_vals = list()\n",
    "ew_rank_difs = list()\n",
    "\n",
    "ew_tp_age_ranks = list()\n",
    "ew_fp_age_ranks = list()\n",
    "ew_tn_age_ranks = list()\n",
    "ew_fn_age_ranks = list()\n",
    "\n",
    "ew_firsts = list()\n",
    "ew_percentages = list()\n",
    "ew_first_rank_difs = list()\n",
    "\n",
    "for bucket in range(2,21):\n",
    "    b_fids = list()\n",
    "    b_preds = list()\n",
    "    b_ranks = list()\n",
    "    b_age_ranks = list()\n",
    "    b_shap_vals = list()\n",
    "    b_rank_difs = list()\n",
    "\n",
    "    b_tp_age_ranks = list()\n",
    "    b_fp_age_ranks = list()\n",
    "    b_tn_age_ranks = list()\n",
    "    b_fn_age_ranks = list()\n",
    "\n",
    "    b_firsts = list()\n",
    "    b_percentages = list()\n",
    "    b_first_rank_difs = list()\n",
    "    for i, seed in enumerate(seeds):\n",
    "        \n",
    "        X2 = X.copy()\n",
    "        bucket_edge = get_bins('uniform',bucket,seed)\n",
    "        X2['Age'] = X2['Age'].apply(lambda age: assign_age(age, bucket_edge)) \n",
    "        np.random.seed(seed)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "        for train_val_idx, test_idx in splitter.split(X2, Y):\n",
    "            X_train_val, X_test = X2.iloc[train_val_idx], X2.iloc[test_idx]\n",
    "            Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "        for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "            X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "            Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx] \n",
    "\n",
    "        \n",
    "        # compute bin edges as shap values for bucketized data\n",
    "        \n",
    "        bucket_shap,bucket_pred = compute_shap(X_train,X_test,Y_train,Y_test,seed)\n",
    "        sv = bucket_shap.values\n",
    "        ew_rank, ew_age_rank = get_ranks(sv)\n",
    "        # Compute fidelity and indices where explanation is the same.\n",
    "\n",
    "        preds = base_preds[i]\n",
    "        sv = bucket_shap.values\n",
    "        base = bucket_shap.base_values\n",
    "        ew_fid, ew_agreed = compute_fidelity(preds,sv,base)\n",
    "\n",
    "        b_fids.append(ew_fid)\n",
    "        b_preds.append(bucket_pred)\n",
    "        b_ranks.append(ew_rank)\n",
    "        b_age_ranks.append(ew_age_rank)\n",
    "        b_shap_vals.append(bucket_shap)\n",
    "\n",
    "        \n",
    "        # Compuute rank shift\n",
    "        base_age_rank = base_age_ranks[i]\n",
    "        rank_dif = [r1-r2 for r1,r2 in zip(base_age_rank,ew_age_rank)]\n",
    "        b_rank_difs.append(rank_dif)\n",
    "\n",
    "        # Errors\n",
    "        tp,fp,tn,fn = get_tfs(sv, Y_test,pred)\n",
    "        tp_rank, tp_age_rank = get_ranks(sv[tp])\n",
    "        fp_rank, fp_age_rank = get_ranks(sv[fp])\n",
    "        tn_rank, tn_age_rank = get_ranks(sv[tn])\n",
    "        fn_rank, fn_age_rank = get_ranks(sv[fn])\n",
    "        b_tp_age_ranks.append(tp_rank)\n",
    "        b_fp_age_ranks.append(fp_rank)\n",
    "        b_tn_age_ranks.append(tn_rank)\n",
    "        b_fn_age_ranks.append(fn_rank)\n",
    "\n",
    "        # First\n",
    "    \n",
    "        ## Indices of first\n",
    "        first = [int(j) for j,v in enumerate(ew_age_rank) if v == 1]\n",
    "        first_rank_dif = [rank_dif[idx] for idx in first]\n",
    "        \n",
    "        b_firsts.append(first)\n",
    "        b_percentages.append(len(first)/len(X_test)*100 )\n",
    "        b_first_rank_difs.append(first_rank_dif)\n",
    "        \n",
    "    ew_fids.append(b_fids)\n",
    "    ew_preds.append(b_preds)\n",
    "    ew_ranks.append(b_ranks)\n",
    "    ew_age_ranks.append(b_age_ranks)\n",
    "    ew_shap_vals.append(b_shap_vals)\n",
    "    ew_rank_difs.append(b_rank_difs)\n",
    "\n",
    "    ew_tp_age_ranks.append(b_tp_age_ranks)\n",
    "    ew_fp_age_ranks.append(b_fp_age_ranks)\n",
    "    ew_tn_age_ranks.append(b_tn_age_ranks)\n",
    "    ew_fn_age_ranks.append(b_fn_age_ranks)\n",
    "\n",
    "    ew_firsts.append(b_firsts)\n",
    "    ew_percentages.append(b_percentages)\n",
    "    ew_first_rank_difs.append(b_first_rank_difs)\n",
    "    # print(f'Average fidelity {sum(cut5_fidelities)/len(cut5_fidelities)*100:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3285d3cd-2d91-47df-92b8-6326532d6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "with open(path + '/Sens_Income_ew_fids_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_fids, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_rank_difs_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_rank_difs, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path + '/Sens_Income_ew_tp_age_ranks_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_tp_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_fp_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_fp_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_tn_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_tn_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_fn_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_fn_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path + '/Sens_Income_ew_firsts_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_firsts, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_percentages_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_percentages, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_Income_ew_first_rank_difs_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_first_rank_difs, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d840d-96d0-438c-ac81-5dc1cb45e175",
   "metadata": {},
   "source": [
    "# Bucket Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "989308fb-007b-4b50-8ba5-10d88146ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "bucket = 10\n",
    "seed = seeds[0]  \n",
    "X2 = X.copy()\n",
    "bucket_edge = get_bins('uniform',bucket,seed)\n",
    "bucket_edge = [math.ceil(e) for e in bucket_edge]\n",
    "\n",
    "with open(path + '/Sens_Income_bucket_edge_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(bucket_edge, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72ea0747-7aec-4fb4-9ffc-17af0d78e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "for train_val_idx, test_idx in splitter.split(X, Y):\n",
    "    X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "    Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "    \n",
    "with open(path + '/Sens_Income_X_test_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(X_test, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71fa767f-fb07-4e2c-8a3e-5d03647a1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_first_freq = dict()\n",
    "# for i in bf:\n",
    "#     b_num = X_test_bucket_indices[i]\n",
    "#     if b_num not in base_first_freq:\n",
    "#         base_first_freq[b_num] = 1\n",
    "#     else:\n",
    "#         base_first_freq[b_num] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91262ab-0d5e-4ddd-9da2-88b3c9264a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# down_all_freq_dict = dict()\n",
    "# up_all_freq_dict = dict()\n",
    "# same_all_freq_dict = dict()\n",
    "# # for i,rank_dif in enumerate(first_rank_shifts):\n",
    "# for i,rank_dif in enumerate(first_rank_shifts):\n",
    "#     for j, dif in enumerate(rank_dif):\n",
    "#         real_idx = bf[j]\n",
    "#         bucket_idx = X_test_bucket_indices[real_idx]\n",
    "#         if dif < 0:\n",
    "            \n",
    "#             if bucket_idx not in down_all_freq_dict:\n",
    "#                 down_all_freq_dict[bucket_idx] = 1\n",
    "#             else:\n",
    "#                 down_all_freq_dict[bucket_idx] += 1\n",
    "#         elif dif > 0:\n",
    "#             if bucket_idx not in up_all_freq_dict:\n",
    "#                 up_all_freq_dict[bucket_idx] = 1\n",
    "#             else:\n",
    "#                 up_all_freq_dict[bucket_idx] += 1\n",
    "#         else:\n",
    "#             if bucket_idx not in same_all_freq_dict:\n",
    "#                 same_all_freq_dict[bucket_idx] = 1\n",
    "#             else:\n",
    "#                 same_all_freq_dict[bucket_idx] += 1\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6defd55-8c4f-4b50-a6d1-e26ef5106dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the total distribution\n",
    "# plt.bar(list(base_first_freq.keys()), list(base_first_freq.values()))\n",
    "# plt.xticks(list(range(len(cut10_bin))), cut10_bin)\n",
    "# plt.xlabel('Buckets')\n",
    "# plt.xlabel('Counts')\n",
    "# plt.title('Distribution of Age Buckets for First ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5d03d26-68e2-496e-adcc-aff40d95570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Attack_Income_first_down_all_freq_dict.pickle', 'wb') as f:\n",
    "#     pickle.dump(down_all_freq_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('Attack_Income_first_same_all_freq_dict.pickle', 'wb') as f:\n",
    "#     pickle.dump(same_all_freq_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eb5ec06-dbef-40dc-b0e5-874d67d25eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(list(down_all_freq_dict.keys()), list(down_all_freq_dict.values()))\n",
    "# down_cnt = sum([v for v in down_all_freq_dict.values()])\n",
    "# plt.xticks(list(range(len(cut10_bin))), cut10_bin)\n",
    "# plt.title('Rank Demotion for buckets')\n",
    "# plt.xlabel('Buckets')\n",
    "# plt.xlabel('Counts')\n",
    "# print(down_cnt)\n",
    "# print(f'precent {down_cnt/len(bf)*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee647bde-213e-45e7-86d9-9eebd84a6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(list(up_all_freq_dict.keys()), list(up_all_freq_dict.values()))\n",
    "# up_cnt = sum([v for v in up_all_freq_dict.values()])\n",
    "# plt.xticks(list(range(len(cut10_bin))), cut10_bin)\n",
    "# plt.title('Rank Promotion for buckets')\n",
    "# plt.xlabel('Buckets')\n",
    "# plt.xlabel('Counts')\n",
    "# print(up_cnt)\n",
    "# print(f'precent {up_cnt/len(X_test)*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afcc4ef3-4900-42b0-b3ad-104f7534a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(list(same_all_freq_dict.keys()), list(same_all_freq_dict.values()))\n",
    "# same_cnt = sum([v for v in same_all_freq_dict.values()])\n",
    "# plt.xticks(list(range(len(cut10_bin))), cut10_bin)\n",
    "# plt.title('No Rank Shift for Buckets')\n",
    "# plt.xlabel('Buckets')\n",
    "# plt.xlabel('Counts')\n",
    "# print(same_cnt)\n",
    "# print(f'precent {same_cnt/len(bf)*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2de5f6f8-0e91-4ad1-a735-36239f5331f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_freq = dict()\n",
    "# for i in X_test_bucket_indices:\n",
    "#     if i not in base_freq:\n",
    "#         base_freq[i] = 1\n",
    "#     else:\n",
    "#         base_freq[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04fe9c80-a0f7-4d6e-86ed-5f4e9fafcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the total distribution\n",
    "# plt.bar(list(base_freq.keys()), list(base_freq.values()))\n",
    "# plt.xticks(list(range(len(cut10_bin))), cut10_bin)\n",
    "# plt.xlabel('Buckets')\n",
    "# plt.xlabel('Counts')\n",
    "# plt.title('Distribution of Age Buckets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06ba6b28-455e-450e-8105-71d9ae0256d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# down_all_freq_dict = dict()\n",
    "# up_all_freq_dict = dict()\n",
    "# same_all_freq_dict = dict()\n",
    "# for i,rank_dif in enumerate(rank_difs):\n",
    "#     for j, dif in enumerate(rank_dif):\n",
    "#         bucket_idx = X_test_bucket_indices[j]\n",
    "#         if dif < 0:\n",
    "            \n",
    "#             if bucket_idx not in down_all_freq_dict:\n",
    "#                 down_all_freq_dict[bucket_idx] = 1\n",
    "#             else:\n",
    "#                 down_all_freq_dict[bucket_idx] += 1\n",
    "#         elif dif > 0:\n",
    "#             if bucket_idx not in up_all_freq_dict:\n",
    "#                 up_all_freq_dict[bucket_idx] = 1\n",
    "#             else:\n",
    "#                 up_all_freq_dict[bucket_idx] += 1\n",
    "#         else:\n",
    "#             if bucket_idx not in same_all_freq_dict:\n",
    "#                 same_all_freq_dict[bucket_idx] = 1\n",
    "#             else:\n",
    "#                 same_all_freq_dict[bucket_idx] += 1\n",
    "            \n",
    "            \n",
    "# with open('Attack_Income_cut5_bins.pickle', 'wb') as f:\n",
    "#     pickle.dump(cut10_bin, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('Attack_Income_down_all_freq_dict.pickle', 'wb') as f:\n",
    "#     pickle.dump(down_all_freq_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('Attack_Income_up_all_freq_dict.pickle', 'wb') as f:\n",
    "#     pickle.dump(up_all_freq_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('Attack_Income_same_all_freq_dict.pickle', 'wb') as f:\n",
    "#     pickle.dump(same_all_freq_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47595963-a52d-4933-ba83-6618602f1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(list(down_all_freq_dict.keys()), list(down_all_freq_dict.values()))\n",
    "# down_cnt = sum([v for v in down_all_freq_dict.values()])\n",
    "# plt.xticks(list(range(len(cut10_bin))), cut10_bin)\n",
    "# plt.title('Rank Demotion for buckets')\n",
    "# plt.xlabel('Buckets')\n",
    "# plt.xlabel('Counts')\n",
    "# print(down_cnt)\n",
    "# print(f'precent {down_cnt/len(X_test)*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cfe4bbd-b83b-4ef0-9164-91dc4d892faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(list(up_all_freq_dict.keys()), list(up_all_freq_dict.values()))\n",
    "# up_cnt = sum([v for v in up_all_freq_dict.values()])\n",
    "# plt.xticks(list(range(len(cut10_bin))), cut10_bin)\n",
    "# plt.title('Rank Promotion for buckets')\n",
    "# plt.xlabel('Buckets')\n",
    "# plt.xlabel('Counts')\n",
    "# print(up_cnt)\n",
    "# print(f'precent {up_cnt/len(X_test)*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ece5a734-67fc-4b18-9883-e84cf037b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.bar(list(same_all_freq_dict.keys()), list(same_all_freq_dict.values()))\n",
    "# same_cnt = sum([v for v in same_all_freq_dict.values()])\n",
    "# plt.xticks(list(range(len(cut10_bin))), cut10_bin)\n",
    "# plt.title('No Rank Shift for Buckets')\n",
    "# plt.xlabel('Buckets')\n",
    "# plt.xlabel('Counts')\n",
    "# print(same_cnt)\n",
    "# print(f'precent {same_cnt/len(X_test)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbad99f-2509-4e4d-bb1a-ad130664d693",
   "metadata": {},
   "source": [
    "# examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ad6e7a9-6d33-439b-81b6-cf71ee894832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # examples\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[0][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[0][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[0][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][i],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][i],max_display=22)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd96c9db-ede2-4af7-bd4b-3f94c38bf53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TPS\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[1][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[1][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[1][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][tps[0][i]],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][tps[0][i]],max_display=22)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bb98b85-eee7-4089-9745-e225321cc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fPS\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[2][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[2][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[2][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][fps[0][i]],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][fps[0][i]],max_display=22)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bd719bc-bee5-401b-9728-043c1a7ec7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tnS\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[3][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[3][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[3][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][tns[0][i]],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][tns[0][i]],max_display=22)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "657771d6-77f4-457c-8b1d-a485907b90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fnS\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[4][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[4][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[4][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][fns[0][i]],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][fns[0][i]],max_display=22)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb2f33-986c-48f4-9fc5-05dafe9d1f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e092d3f-3992-4145-b2e6-ffdb66096b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = [m for m,s in ew_bucket_downs]\n",
    "# stds = [s for m,s in ew_bucket_downs]\n",
    "\n",
    "# decrease_bin = list()\n",
    "# for decrease_idx in range(4):\n",
    "#     m = [mean[decrease_idx] for mean in means] \n",
    "#     decrease_bin.append(m)\n",
    "\n",
    "# colors = ['r','b','g','c','m']\n",
    "# p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "# for i in range(len(decrease_bin)):\n",
    "#     plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "# plt.xticks(range(2,21))\n",
    "# plt.title('Rank shifts where Rank of Age is 1')\n",
    "# plt.xlabel('Number of Buckets')\n",
    "# plt.xlabel('Proportion')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f0e7fc4-9603-4df0-aa21-4f954004d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# with open('Attack_Income_ew_downs_tp.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_bucket_tp_downs, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('Attack_Income_ew_downs_fp.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_bucket_fp_downs, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('Attack_Income_ew_downs_tn.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_bucket_tn_downs, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open('Attack_Income_ew_downs_fn.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_bucket_fn_downs, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b129ac14-411d-423f-adcd-4125a588b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = [m for m,s in ew_bucket_tp_downs]\n",
    "# stds = [s for m,s in ew_bucket_tp_downs]\n",
    "\n",
    "# decrease_bin = list()\n",
    "# for decrease_idx in range(4):\n",
    "#     m = [mean[decrease_idx] for mean in means] \n",
    "#     decrease_bin.append(m)\n",
    "\n",
    "# colors = ['r','b','g','c','m']\n",
    "# p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "# for i in range(len(decrease_bin)):\n",
    "#     plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "# plt.xticks(range(2,21))\n",
    "# plt.title('TP Rank shifts where Rank of Age is 1')\n",
    "# plt.xlabel('Number of Buckets')\n",
    "# plt.xlabel('Proportion')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dc58cf0-1554-4963-b3f5-9f1300ccc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = [m for m,s in ew_bucket_fp_downs]\n",
    "# stds = [s for m,s in ew_bucket_fp_downs]\n",
    "\n",
    "# decrease_bin = list()\n",
    "# for decrease_idx in range(4):\n",
    "#     m = [mean[decrease_idx] for mean in means] \n",
    "#     decrease_bin.append(m)\n",
    "\n",
    "# colors = ['r','b','g','c','m']\n",
    "# p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "# for i in range(len(decrease_bin)):\n",
    "#     plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "# plt.xticks(range(2,21))\n",
    "# plt.title('FP Rank shifts where Rank of Age is 1')\n",
    "# plt.xlabel('Number of Buckets')\n",
    "# plt.xlabel('Proportion')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "329c8569-a641-41df-bfcb-22db788b8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = [m for m,s in ew_bucket_tn_downs]\n",
    "# stds = [s for m,s in ew_bucket_tn_downs]\n",
    "\n",
    "# decrease_bin = list()\n",
    "# for decrease_idx in range(4):\n",
    "#     m = [mean[decrease_idx] for mean in means] \n",
    "#     decrease_bin.append(m)\n",
    "\n",
    "# colors = ['r','b','g','c','m']\n",
    "# p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "# for i in range(len(decrease_bin)):\n",
    "#     plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "# plt.xticks(range(2,21))\n",
    "# plt.title('TN Rank shifts where Rank of Age is 1')\n",
    "# plt.xlabel('Number of Buckets')\n",
    "# plt.xlabel('Proportion')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "106099ac-f3e5-4cf2-87ab-07290fd59e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = [m for m,s in ew_bucket_fn_downs]\n",
    "# stds = [s for m,s in ew_bucket_fn_downs]\n",
    "\n",
    "# decrease_bin = list()\n",
    "# for decrease_idx in range(4):\n",
    "#     m = [mean[decrease_idx] for mean in means] \n",
    "#     decrease_bin.append(m)\n",
    "\n",
    "# colors = ['r','b','g','c','m']\n",
    "# p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "# for i in range(len(decrease_bin)):\n",
    "#     plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "# plt.xticks(range(2,21))\n",
    "# plt.title('FN Rank shifts where Rank of Age is 1')\n",
    "# plt.xlabel('Number of Buckets')\n",
    "# plt.xlabel('Proportion')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d90a3-2ff3-4acb-8b9a-2c6a49918695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629945eb-cd27-424e-aa9f-91f301ba0234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4379f97-1e7a-4b22-b97c-72c65d8739b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
