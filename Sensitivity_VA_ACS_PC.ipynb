{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8109a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = \"ignore\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from shap import GPUTreeExplainer\n",
    "from matplotlib.ticker import MaxNLocator,MultipleLocator\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.special import expit\n",
    "\n",
    "import mlresearch\n",
    "mlresearch.utils.set_matplotlib_style()\n",
    "from mlresearch.utils import set_matplotlib_style\n",
    "set_matplotlib_style(font_size=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36a1592-b0c5-4100-b424-b7db710762cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n",
      "1.6.1\n",
      "2.1.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__) #1.26.4\n",
    "# print(shap.__version__) #0.46.1.dev86\n",
    "print(sklearn.__version__) #1.6.0\n",
    "print(xgb.__version__) #1.7.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f97860-e454-41e5-b150-445368cff597",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cd23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use White alone & African American only \n",
    "FEAT_CNT = 16\n",
    "STATE = 'VA'\n",
    "FOLDS = 5\n",
    "seeds = [0,21,42,63,84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21fb6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_cols =['Occupation', 'Marriage','Place of Birth','Sex', 'Race']\n",
    "\n",
    "with open(file=f'dataset/ACS_PublicCoverage_{STATE}.pickle', mode='rb') as f:\n",
    "    df=pickle.load(f)\n",
    "columns = df.columns\n",
    "with pd.option_context('future.no_silent_downcasting', True):\n",
    "    df['Sex'].replace( {'Female':0.0},inplace = True)\n",
    "    df['Sex'].replace({'Male':1.0}, inplace = True)\n",
    "    df['Nativity'].replace( {'Native':1.0},inplace = True)\n",
    "    df['Nativity'].replace({'Foreign born':0.0}, inplace = True)\n",
    "    df['Disability'].replace( {'No':0.0},inplace = True)\n",
    "    df['Disability'].replace({'Yes':1.0}, inplace = True)\n",
    "    df['Hearing Difficulty'].replace( {'No':0.0},inplace = True)\n",
    "    df['Hearing Difficulty'].replace({'Yes':1.0}, inplace = True)\n",
    "    df['Vision Difficulty'].replace( {'No':0.0},inplace = True)\n",
    "    df['Vision Difficulty'].replace({'Yes':1.0}, inplace = True)\n",
    "    df['Cognitive Difficulty'].replace( {'No':0.0},inplace = True)\n",
    "    df['Cognitive Difficulty'].replace({'Yes':1.0}, inplace = True)\n",
    "    \n",
    "X = df.iloc[:, 0:FEAT_CNT]\n",
    "Y = df.iloc[:, FEAT_CNT]\n",
    "\n",
    "category_col =['Ancestry recode', 'Citizenship', 'Marriage','Employment Status','Mobility status', 'Military Service','Race']\n",
    "X = pd.get_dummies(X, columns=category_col, drop_first=True)\n",
    "for c in X.columns:\n",
    "    X[c] = X[c].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6348785-6c76-46f4-89ed-fb2570018c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5fb1a-7d9d-4f4a-a71d-a65f066d472b",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b12389d-a0f6-4dbc-b1f8-5b61b6a5dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(strategy, bin_size, seed):\n",
    "    ## compute bin boundary\n",
    "    np.random.seed(seed)\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "    for train_val_idx, test_idx in splitter.split(X, Y):\n",
    "        X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "        Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "        X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "        Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx] \n",
    "    kd = KBinsDiscretizer(n_bins=bin_size, encode='ordinal', strategy=strategy)\n",
    "\n",
    "    kd.fit(X_train)\n",
    "    bin_boundaries = kd.bin_edges_[0]\n",
    "\n",
    "    return bin_boundaries\n",
    "def assign_age(age,bin_edges):\n",
    "    # Assign age to a median of the bin_edges\n",
    "    for idx in range(len(bin_edges)-1):\n",
    "        if age == bin_edges[-1]:\n",
    "            median = (bin_edges[-1] + bin_edges[-2])/2\n",
    "        elif bin_edges[idx] <= age and age < bin_edges[idx+1]:\n",
    "            median = (bin_edges[idx] + bin_edges[idx+1])/2\n",
    "    return median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb95a13b-fa1a-41f7-a814-766d76899de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shap(X_train,X_test,Y_train,Y_test, seed):\n",
    "\n",
    "    print('**********START**********')\n",
    "    # Train model on new data\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "        'classifier__max_depth': [3, 5, 7,9,11],          # Maximum tree depth\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage \n",
    "        'classifier__colsample_bytree': [0.8, 1.0],  # Subsample ratio of columns for each tree\n",
    "        'classifier__gamma': [0, 0.1, 0.2],          # Minimum loss reduction for a split\n",
    "    }\n",
    "    model = xgb.XGBClassifier(random_state=seed)\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid,              # 3-fold cross-validation\n",
    "        scoring='f1',   # Evaluation metric\n",
    "        n_jobs=-1,            # Use all processors\n",
    "        verbose=1             # Print progress\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "        \n",
    "    # Extract the best model\n",
    "    model = grid_search.best_estimator_\n",
    "\n",
    "    explainer = GPUTreeExplainer(model,X_train,feature_perturbation = 'interventional')\n",
    "    shap_values = explainer(X_test)\n",
    "    pred = best_model.predict(X_test)\n",
    "    return shap_values, pred\n",
    "    \n",
    "def get_tfs(shap_vals,Y_true, pred):\n",
    "    # Compute indices of errors\n",
    "    TP_i = np.where((Y_true == 1.0) & (pred == 1.0))[0]  # True Positives\n",
    "    FP_i = np.where((Y_true == 0.0) & (pred == 1.0))[0]  # False Positives\n",
    "    TN_i = np.where((Y_true == 0.0) & (pred == 0.0))[0]  # True Negatives\n",
    "    FN_i = np.where((Y_true == 1.0) & (pred == 0.0))[0]  # False Negatives\n",
    "    return TP_i,FP_i,TN_i,FN_i\n",
    "\n",
    "def get_ranks(shap_vals):\n",
    "\n",
    "    # avg_shap = np.mean(np.abs(shap_vals), axis=0)\n",
    "    # Compute rankings row-wise\n",
    "    sorted_indices = np.argsort(-np.abs(shap_vals), axis=1)  # Indices of absolute values in descending order\n",
    "    rank = np.empty_like(sorted_indices)             # Create an empty array of the same shape\n",
    "    rows, cols = shap_vals.shape                            # Get the shape of the array\n",
    "    rank[np.arange(rows)[:, None], sorted_indices] = np.arange(1, cols + 1)  # Assign ranks row-wise\n",
    "\n",
    "    target_rank = rank[:,0]\n",
    "\n",
    "    return rank,target_rank\n",
    "\n",
    "def compare_shap(s1, s2, plot = False):\n",
    "    shap_dif = np.abs(np.subtract(s1[:, 0], s2[:, 0])) #/ np.abs(s1[:, 0])\n",
    "    return shap_dif\n",
    "\n",
    "def compare_ranks(rank1, rank2,agreed_idx, plot = False):\n",
    "\n",
    "    age_ranks1,age_ranks2 = rank1[:,0],rank2[:,0]\n",
    "    kendalls = [kendalltau(r1,r2,method='exact').statistic for r1,r2 in zip(rank1,rank2)]\n",
    "    full_abs_rank_dif = [abs(r1-r2) for r1,r2 in zip(age_ranks1,age_ranks2)]\n",
    "    full_rank_dif = [r1-r2 for r1,r2 in zip(age_ranks1,age_ranks2)]\n",
    "    if plot:\n",
    "        plot_frequency(full_rank_dif)\n",
    "    top_4_idx = list()\n",
    "    # Compute indices where rank differences are the largest and the smallest\n",
    "    largest = list(np.argsort(full_rank_dif)[-100:][::-1])\n",
    "    smallest = list(np.argsort(full_rank_dif)[:100])\n",
    "    for s in smallest:\n",
    "        if len(top_4_idx) < 4 and s in agreed_idx:\n",
    "            top_4_idx.append(s)\n",
    "    return full_rank_dif, full_abs_rank_dif, top_4_idx\n",
    "    \n",
    "def compare_results(s1, s2, rank1, rank2,agreed_idx, plot = False):\n",
    "    # Compare baselines\n",
    "    s1,s2 = s1.values, s2.values\n",
    "    \n",
    "    shap_dif = compare_shap(s1,s2)\n",
    "    full_rank_dif, full_abs_rank_dif, top_4_idx = compare_ranks(rank1,rank2,agreed_idx,plot=plot)\n",
    "    kendalls = [kendalltau(r1,r2,method='exact').statistic for r1,r2 in zip(rank1,rank2)]\n",
    "        \n",
    "\n",
    "    return top_4_idx, shap_dif, full_rank_dif, kendalls\n",
    "\n",
    "\n",
    "\n",
    "def compute_fidelity(pred, sv, base):\n",
    "\n",
    "    sv_sums = expit(np.sum(sv, axis=1)+base)\n",
    "    binary_predictions = (sv_sums > 0.5).astype(float)\n",
    "    fidelity = np.mean(binary_predictions == pred)\n",
    "    match_idx = np.where(binary_predictions == pred)[0]\n",
    "        \n",
    "    return fidelity,match_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e55d4",
   "metadata": {},
   "source": [
    "## Train the model with plain Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee15125f-6596-4bd0-ba71-c8a45d2efd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████▍                                     | 1/5 [00:23<01:33, 23.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████▊                            | 2/5 [00:38<00:56, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████▏                  | 3/5 [00:54<00:34, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████▌         | 4/5 [01:09<00:16, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 5/5 [01:24<00:00, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall average acc: 86.41 average f1s : 58.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# base_models = list()\n",
    "base_shap_vals = list()\n",
    "base_preds = list()\n",
    "base_accs = list() \n",
    "base_f1s = list()\n",
    "base_ranks = list()\n",
    "base_age_ranks = list()\n",
    "\n",
    "base_tp_idx = list()\n",
    "base_fp_idx = list()\n",
    "base_tn_idx = list()\n",
    "base_fn_idx = list()\n",
    "\n",
    "base_tp_age_ranks = list()\n",
    "base_fp_age_ranks = list()\n",
    "base_tn_age_ranks = list()\n",
    "base_fn_age_ranks = list()\n",
    "\n",
    "base_firsts = list()\n",
    "base_percentages = list()\n",
    "for seed in tqdm(seeds):\n",
    "    np.random.seed(seed)\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "    for train_val_idx, test_idx in splitter.split(X, Y):\n",
    "        X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "        Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "        X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "        Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx]\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "        'classifier__max_depth': [3, 5, 7,9,11],          # Maximum tree depth\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage \n",
    "        'classifier__colsample_bytree': [0.8, 1.0],  # Subsample ratio of columns for each tree\n",
    "        'classifier__gamma': [0, 0.1, 0.2],          # Minimum loss reduction for a split\n",
    "    }\n",
    "    model = xgb.XGBClassifier(random_state=seed)\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid,              # 3-fold cross-validation\n",
    "        scoring='f1',   # Evaluation metric\n",
    "        n_jobs=13,            # Use all processors\n",
    "        verbose=1             # Print progress\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    explainer = GPUTreeExplainer(best_model,X_train, feature_perturbation='interventional') \n",
    "    shap_values = explainer(X_test)\n",
    "    \n",
    "    sv = shap_values.values\n",
    "    base_rank,age_rank= get_ranks(sv)\n",
    "    base_ranks.append(base_rank)\n",
    "    \n",
    "    pred = best_model.predict(X_test)\n",
    "    # base_models.append(best_model)\n",
    "    base_shap_vals.append(shap_values)\n",
    "    base_preds.append(pred)\n",
    "    base_accs.append(accuracy_score(Y_test,pred)*100)\n",
    "    base_f1s.append(f1_score(Y_test,pred)*100)\n",
    "    base_ranks.append(base_rank)\n",
    "    base_age_ranks.append(age_rank)\n",
    "\n",
    "print(f'Overall average acc: {sum(base_accs)/len(base_accs):.2f} average f1s : {sum(base_f1s)/len(base_f1s):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f221e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "path = './results'\n",
    "if not os.path.exists(path):\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(path)\n",
    "\n",
    "# # save\n",
    "with open(path + '/Sens_PC_base_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_PC_base_accs_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_accs, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_PC_base_f1s_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_f1s, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_PC_base_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open(path + '/Sens_Income_base_tp_age_ranks_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(base_tp_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_base_fp_age_ranks_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(base_fp_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_base_tn_age_ranks_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(base_tn_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_base_fn_age_ranks_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(base_fn_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open(path + '/Sens_Income_base_firsts_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(base_firsts, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_base_percentages_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(base_percentages, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb95cba-8e8c-4ea5-8a67-6b8d7aa65e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e5c3611",
   "metadata": {},
   "source": [
    " # Equi Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050bf71c-5fab-41fa-9844-8e9ee18b07a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    }
   ],
   "source": [
    "ew_fids = list()\n",
    "ew_preds = list()\n",
    "ew_ranks = list()\n",
    "ew_age_ranks = list()\n",
    "ew_shap_vals = list()\n",
    "ew_rank_difs = list()\n",
    "\n",
    "ew_tp_ranks = list()\n",
    "ew_fp_ranks = list()\n",
    "ew_tn_ranks = list()\n",
    "ew_fn_ranks = list()\n",
    "\n",
    "ew_firsts = list()\n",
    "ew_percentages = list()\n",
    "ew_first_rank_difs = list()\n",
    "\n",
    "for bucket in range(2,21):\n",
    "    b_fids = list()\n",
    "    b_preds = list()\n",
    "    b_ranks = list()\n",
    "    b_age_ranks = list()\n",
    "    b_shap_vals = list()\n",
    "    b_rank_difs = list()\n",
    "\n",
    "    b_tp_ranks = list()\n",
    "    b_fp_ranks = list()\n",
    "    b_tn_ranks = list()\n",
    "    b_fn_ranks = list()\n",
    "\n",
    "    b_firsts = list()\n",
    "    b_percentages = list()\n",
    "    b_first_rank_difs = list()\n",
    "    for i, seed in enumerate(seeds):\n",
    "        \n",
    "        X2 = X.copy()\n",
    "        bucket_edge = get_bins('uniform',bucket,seed)\n",
    "        X2['Age'] = X2['Age'].apply(lambda age: assign_age(age, bucket_edge)) \n",
    "        np.random.seed(seed)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "        for train_val_idx, test_idx in splitter.split(X2, Y):\n",
    "            X_train_val, X_test = X2.iloc[train_val_idx], X2.iloc[test_idx]\n",
    "            Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "        for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "            X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "            Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx] \n",
    "\n",
    "        \n",
    "        # compute bin edges as shap values for bucketized data\n",
    "        \n",
    "        bucket_shap,bucket_pred = compute_shap(X_train,X_test,Y_train,Y_test,seed)\n",
    "        sv = bucket_shap.values\n",
    "        ew_rank, ew_age_rank = get_ranks(sv)\n",
    "        # Compute fidelity and indices where explanation is the same.\n",
    "\n",
    "        preds = base_preds[i]\n",
    "        sv = bucket_shap.values\n",
    "        base = bucket_shap.base_values\n",
    "        ew_fid, ew_agreed = compute_fidelity(preds,sv,base)\n",
    "\n",
    "        b_fids.append(ew_fid)\n",
    "        b_preds.append(bucket_pred)\n",
    "        b_ranks.append(ew_rank)\n",
    "        b_age_ranks.append(ew_age_rank)\n",
    "        b_shap_vals.append(bucket_shap)\n",
    "\n",
    "        \n",
    "        # Compuute rank shift\n",
    "        base_age_rank = base_age_ranks[i]\n",
    "        rank_dif = [r1-r2 for r1,r2 in zip(base_age_rank,ew_age_rank)]\n",
    "        b_rank_difs.append(rank_dif)\n",
    "\n",
    "        \n",
    "    ew_fids.append(b_fids)\n",
    "    ew_preds.append(b_preds)\n",
    "    ew_ranks.append(b_ranks)\n",
    "    ew_age_ranks.append(b_age_ranks)\n",
    "    ew_shap_vals.append(b_shap_vals)\n",
    "    ew_rank_difs.append(b_rank_difs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcbc43ab-87c5-4ead-a7d5-42f2ecf0f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "with open(path + '/Sens_PC_ew_fids_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_fids, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_PC_ew_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_PC_ew_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_age_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_PC_ew_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Sens_PC_ew_rank_difs_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(ew_rank_difs, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open(path + '/Sens_Income_ew_tp_ranks_ranks_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_tp_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_ew_fp_age_ranks_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_fp_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_ew_tn_age_ranks_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_tn_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_ew_fn_age_ranks_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_fn_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open(path + '/Sens_Income_ew_firsts_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_firsts, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_ew_percentages_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_percentages, f, pickle.HIGHEST_PROTOCOL)\n",
    "# with open(path + '/Sens_Income_ew_first_rank_difs_cv.pickle', 'wb') as f:\n",
    "#     pickle.dump(ew_first_rank_difs, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b121af3-b72b-40c6-9c04-1b916408ce2c",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a6ae5-9060-4f2e-ac71-2a03b106c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # examples\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[0][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[0][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[0][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][i],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][i],max_display=22)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559a3b6-e992-425b-a572-61b9927ae2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TPS\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[1][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[1][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[1][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][tps[0][i]],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][tps[0][i]],max_display=22)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae5a17-53d3-42ba-9e2f-c3cc1b681bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fPS\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[2][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[2][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[2][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][fps[0][i]],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][fps[0][i]],max_display=22)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dc404-0bc5-4cfe-b033-70070da6b491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # tnS\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[3][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[3][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[3][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][tns[0][i]],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][tns[0][i]],max_display=22)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632c229-9e0f-4d9a-a7d1-092a86a9e9b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # fnS\n",
    "# first_dict = dict()\n",
    "# for idx,shift in enumerate(examples[4][0][0]):\n",
    "#     if shift not in first_dict:\n",
    "#         first_dict[shift] = [examples[4][1][0][idx]]\n",
    "#     else:\n",
    "#         first_dict[shift].append(examples[4][1][0][idx])\n",
    "# smallest = min(list(first_dict.keys()))\n",
    "# print(smallest)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(base_result['shap_vals'][0][fns[0][i]],max_display=22)\n",
    "# for it,i in enumerate(first_dict[smallest]):\n",
    "#     shap.plots.bar(example_results[0]['shap_vals'][0][fns[0][i]],max_display=22)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e092d3f-3992-4145-b2e6-ffdb66096b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [m for m,s in ew_bucket_downs]\n",
    "stds = [s for m,s in ew_bucket_downs]\n",
    "\n",
    "decrease_bin = list()\n",
    "for decrease_idx in range(4):\n",
    "    m = [mean[decrease_idx] for mean in means] \n",
    "    decrease_bin.append(m)\n",
    "\n",
    "colors = ['r','b','g','c','m']\n",
    "p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "for i in range(len(decrease_bin)):\n",
    "    plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "plt.xticks(range(2,21))\n",
    "plt.title('Rank shifts where Rank of Age is 1')\n",
    "plt.xlabel('Number of Buckets')\n",
    "plt.xlabel('Proportion')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129ac14-411d-423f-adcd-4125a588b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [m for m,s in ew_bucket_tp_downs]\n",
    "stds = [s for m,s in ew_bucket_tp_downs]\n",
    "\n",
    "decrease_bin = list()\n",
    "for decrease_idx in range(4):\n",
    "    m = [mean[decrease_idx] for mean in means] \n",
    "    decrease_bin.append(m)\n",
    "\n",
    "colors = ['r','b','g','c','m']\n",
    "p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "for i in range(len(decrease_bin)):\n",
    "    plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "plt.xticks(range(2,21))\n",
    "plt.title('TP Rank shifts where Rank of Age is 1')\n",
    "plt.xlabel('Number of Buckets')\n",
    "plt.xlabel('Proportion')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c8569-a641-41df-bfcb-22db788b8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [m for m,s in ew_bucket_fp_downs]\n",
    "stds = [s for m,s in ew_bucket_fp_downs]\n",
    "\n",
    "decrease_bin = list()\n",
    "for decrease_idx in range(4):\n",
    "    m = [mean[decrease_idx] for mean in means] \n",
    "    decrease_bin.append(m)\n",
    "\n",
    "colors = ['r','b','g','c','m']\n",
    "p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "for i in range(len(decrease_bin)):\n",
    "    plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "plt.xticks(range(2,21))\n",
    "plt.title('FP Rank shifts where Rank of Age is 1')\n",
    "plt.xlabel('Number of Buckets')\n",
    "plt.xlabel('Proportion')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106099ac-f3e5-4cf2-87ab-07290fd59e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [m for m,s in ew_bucket_tn_downs]\n",
    "stds = [s for m,s in ew_bucket_tn_downs]\n",
    "\n",
    "decrease_bin = list()\n",
    "for decrease_idx in range(4):\n",
    "    m = [mean[decrease_idx] for mean in means] \n",
    "    decrease_bin.append(m)\n",
    "\n",
    "colors = ['r','b','g','c','m']\n",
    "p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "for i in range(len(decrease_bin)):\n",
    "    plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "plt.xticks(range(2,21))\n",
    "plt.title('TN Rank shifts where Rank of Age is 1')\n",
    "plt.xlabel('Number of Buckets')\n",
    "plt.xlabel('Proportion')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d90a3-2ff3-4acb-8b9a-2c6a49918695",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [m for m,s in ew_bucket_fn_downs]\n",
    "stds = [s for m,s in ew_bucket_fn_downs]\n",
    "\n",
    "decrease_bin = list()\n",
    "for decrease_idx in range(4):\n",
    "    m = [mean[decrease_idx] for mean in means] \n",
    "    decrease_bin.append(m)\n",
    "\n",
    "colors = ['r','b','g','c','m']\n",
    "p_labels = ['Rank 2','Rank 3','Rank 4', 'Rank 5 or below']\n",
    "\n",
    "for i in range(len(decrease_bin)):\n",
    "    plt.plot(list(range(2,21)), decrease_bin[i],color=colors[i],label=p_labels[i],ls='-')\n",
    "\n",
    "plt.xticks(range(2,21))\n",
    "plt.title('FN Rank shifts where Rank of Age is 1')\n",
    "plt.xlabel('Number of Buckets')\n",
    "plt.xlabel('Proportion')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabcdc7-6ff7-4325-b313-cfd5b799f958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4379f97-1e7a-4b22-b97c-72c65d8739b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
