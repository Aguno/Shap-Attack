{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6097484e-ec87-4f81-9306-72dc8f7f7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import os\n",
    "os.environ['PYTHONWARNINGS'] = \"ignore\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from shap import GPUTreeExplainer\n",
    "from matplotlib.ticker import MaxNLocator,MultipleLocator\n",
    "from scipy.stats import kendalltau\n",
    "from scipy.special import expit\n",
    "\n",
    "import mlresearch\n",
    "mlresearch.utils.set_matplotlib_style()\n",
    "from mlresearch.utils import set_matplotlib_style\n",
    "set_matplotlib_style(font_size=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f97860-e454-41e5-b150-445368cff597",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10cd23aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use White alone & African American only \n",
    "FEAT_CNT = 8\n",
    "STATE = 'VA'\n",
    "FOLDS = 5\n",
    "seeds = [0,21,42,63,84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ee20d7-36f1-4810-8a6b-ea8abf3b112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n",
      "1.6.1\n",
      "2.1.4\n",
      "3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__) #1.26.4\n",
    "# print(shap.__version__) #0.46.1.dev86\n",
    "print(sklearn.__version__) #1.6.0\n",
    "print(xgb.__version__) #1.7.6\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21fb6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols =['Occupation', 'Marriage','Place of Birth','Sex', 'Race']\n",
    "\n",
    "with open(file=f'dataset/ACS_Income_{STATE}.pickle', mode='rb') as f:\n",
    "    df=pickle.load(f)\n",
    "columns = df.columns\n",
    "with pd.option_context('future.no_silent_downcasting', True):\n",
    "    df.replace([' <=50K',' >50K'],\n",
    "                 [0,1], inplace = True)\n",
    "    df['Sex'].replace( {'Female':0.0},inplace = True)\n",
    "    df['Sex'].replace({'Male':1.0}, inplace = True)\n",
    "df['Race'] = df['Race'].str.strip().str.lower()\n",
    "race_used = ['asian alone', 'black or african american alone', 'white alone']\n",
    "\n",
    "# Replace values not in race_used with 'Other'\n",
    "df['Race'] = df['Race'].apply(lambda x: x if x in race_used else 'other')\n",
    "X = df.iloc[:, 0:FEAT_CNT]\n",
    "Y = df.iloc[:, FEAT_CNT]\n",
    "\n",
    "category_col =['Occupation', 'Marriage','Place of Birth', 'Race']\n",
    "X = pd.get_dummies(X, columns=category_col, drop_first=False)\n",
    "for c in X.columns:\n",
    "    X[c] = X[c].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8eb8415-84de-4aac-9698-a6530ae05fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['black or african american alone', 'white alone', 'other',\n",
       "       'asian alone'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Race'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade2b94e-fd7a-40ba-a0ea-6134bc2b6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2 buckets\n",
    "    White, Black + Asian + Other\n",
    "    White + Black , Asian + Other ( Number-wise majority vs minority)\n",
    "    White + Asian, Black + Other (Privileged vs unprivileged)\n",
    "3 buckets\n",
    "    White, Black, Asian + Other\n",
    "    White, Asian, Black + Other\n",
    "'''\n",
    "\n",
    "white, black, asian, other =  'Race_white alone','Race_black or african american alone', 'Race_asian alone', 'Race_other'\n",
    "all_races = [white, black, asian,other]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5fb1a-7d9d-4f4a-a71d-a65f066d472b",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f2a80d4d-a5ee-47b0-8814-2a843768f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_race(X, target_race):\n",
    "    X2 = X.copy()\n",
    "    # 1) 먼저 모든 race 컬럼을 0으로 초기화\n",
    "    for r in all_races:\n",
    "        X2[r] = 0.0\n",
    "\n",
    "    # 2) 원본에서 target_race==1 이었던 행들은 다시 1로 돌려놓기\n",
    "    mask_target = X[target_race] == 1\n",
    "    X2.loc[mask_target, target_race] = 1.0\n",
    "\n",
    "    # 3) target_race가 아닌 나머지 행들은 모두 'other'로 표시\n",
    "    if target_race != 'other':\n",
    "        X2.loc[~mask_target, 'other'] = 1.0\n",
    "    else:\n",
    "        mask_non_other = X[['white','black','asian']].any(axis=1)\n",
    "        X2.loc[mask_non_other, 'other'] = 0.0\n",
    "        X2.loc[~mask_non_other, 'other'] = 1.0\n",
    "\n",
    "    return X2\n",
    "\n",
    "def sum_race_shaps(shap_vals):\n",
    "    \n",
    "    all_races = [white,black,asian,other]\n",
    "    all_race_idx = [list(X.columns).index(race) for race in all_races]\n",
    "\n",
    "    # Step 1: Compute the sum of specified columns for each row\n",
    "    new_column =shap_vals[:, all_race_idx].sum(axis=1)\n",
    "    \n",
    "    # Step 2: Add the new column to the array\n",
    "    shap_vals = np.hstack((shap_vals, new_column.reshape(-1, 1)))\n",
    "    \n",
    "    # Step 3: Delete the specified columns\n",
    "    shap_vals = np.delete(shap_vals, all_race_idx, axis=1)\n",
    "    return shap_vals\n",
    "def get_ranks(shap_vals):\n",
    "\n",
    "    sorted_indices = np.argsort(-np.abs(shap_vals), axis=1)  # Indices of absolute values in descending order\n",
    "    rank = np.empty_like(sorted_indices)             # Create an empty array of the same shape\n",
    "    rows, cols = shap_vals.shape                            # Get the shape of the array\n",
    "    rank[np.arange(rows)[:, None], sorted_indices] = np.arange(1, cols + 1)  # Assign ranks row-wise\n",
    "\n",
    "    target_rank = rank[:,-1]\n",
    "\n",
    "    return rank,target_rank\n",
    "\n",
    "def compute_shap(X_train,X_test,Y_train,Y_test, seed):\n",
    "\n",
    "    print('**********START**********')\n",
    "    # Train model on new data\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "        'classifier__max_depth': [3, 5, 7,9,11],          # Maximum tree depth\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage \n",
    "        'classifier__colsample_bytree': [0.8, 1.0],  # Subsample ratio of columns for each tree\n",
    "        'classifier__gamma': [0, 0.1, 0.2],          # Minimum loss reduction for a split\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(random_state=seed)\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid,              # 3-fold cross-validation\n",
    "        scoring='f1',   # Evaluation metric\n",
    "        n_jobs=-1,            # Use all processors\n",
    "        verbose=1             # Print progress\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    explainer = GPUTreeExplainer(best_model,X_train,feature_perturbation = 'interventional')\n",
    "    shap_values = explainer(X_test)\n",
    "    pred = best_model.predict(X_test)\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "def compute_fidelity(pred, sv, base):\n",
    "\n",
    "    sv_sums = expit(np.sum(sv, axis=1)+base)\n",
    "    binary_predictions = (sv_sums > 0.5).astype(float)\n",
    "    fidelity = np.mean(binary_predictions == pred)\n",
    "    match_idx = np.where(binary_predictions == pred)[0]\n",
    "        \n",
    "    return fidelity,match_idx\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e55d4",
   "metadata": {},
   "source": [
    "## Train the model with plain Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ee15125f-6596-4bd0-ba71-c8a45d2efd9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████▍                                     | 1/5 [00:21<01:24, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████▊                            | 2/5 [00:38<00:56, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████▏                  | 3/5 [00:55<00:36, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████▌         | 4/5 [01:12<00:17, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 5/5 [01:29<00:00, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall average acc: 79.47 average f1s : 76.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_shap_vals = list()\n",
    "base_preds = list()\n",
    "base_accs = list() \n",
    "base_f1s = list()\n",
    "base_ranks = list()\n",
    "base_race_ranks = list()\n",
    "\n",
    "base_firsts = list()\n",
    "base_percentages = list()\n",
    "base_first_race_shap_vals = list()\n",
    "\n",
    "base_models = list()\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    np.random.seed(seed)\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "    for train_val_idx, test_idx in splitter.split(X, Y):\n",
    "        X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
    "        Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "    for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "        X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "        Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx]\n",
    "    param_grid = {\n",
    "        'classifier__n_estimators': [50, 100, 200],  # Number of boosting rounds\n",
    "        'classifier__max_depth': [3, 5, 7,9,11],          # Maximum tree depth\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage \n",
    "        'classifier__colsample_bytree': [0.8, 1.0],  # Subsample ratio of columns for each tree\n",
    "        'classifier__gamma': [0, 0.1, 0.2],          # Minimum loss reduction for a split\n",
    "    }\n",
    "    model = xgb.XGBClassifier(random_state=seed)\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid,              # 3-fold cross-validation\n",
    "        scoring='f1',   # Evaluation metric\n",
    "        n_jobs=13,            # Use all processors\n",
    "        verbose=1             # Print progress\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    base_models.append(best_model)\n",
    "    explainer = GPUTreeExplainer(best_model,X_train, feature_perturbation='interventional') \n",
    "    shap_values = explainer(X_test)\n",
    "    \n",
    "    sv = sum_race_shaps(shap_values.values)\n",
    "    race_sv = sv[:,-1]\n",
    "    base_rank,race_rank= get_ranks(sv)\n",
    "    base_ranks.append(base_rank)\n",
    "    \n",
    "    pred = best_model.predict(X_test)\n",
    "    # base_models.append(best_model)\n",
    "    base_shap_vals.append(race_sv)\n",
    "    base_preds.append(pred)\n",
    "    base_accs.append(accuracy_score(Y_test,pred)*100)\n",
    "    base_f1s.append(f1_score(Y_test,pred)*100)\n",
    "    base_ranks.append(base_rank)\n",
    "    base_race_ranks.append(race_rank)\n",
    "    # First\n",
    "\n",
    "    ## Indices of first\n",
    "    first = [int(j) for j,v in enumerate(race_rank) if v == 1]\n",
    "    base_firsts.append(first)\n",
    "    base_percentages.append(len(first)/len(X_test) *100)\n",
    "    base_first_race_shap_vals.append(race_sv[first])\n",
    "print(f'Overall average acc: {sum(base_accs)/len(base_accs):.2f} average f1s : {sum(base_f1s)/len(base_f1s):.2f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f5e06840-6c00-408c-8319-61612c4d0298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 19,  9, ..., 11,  3,  9])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "14a8eecf-f6d2-4dc8-b2ea-81d140448d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "path = './results'\n",
    "if not os.path.exists(path):\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(path)\n",
    "\n",
    "# # save\n",
    "with open(path + '/Bucket_OVA_Income_base_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_base_accs_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_accs, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_base_f1s_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_f1s, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_base_age_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_race_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path + '/Bucket_OVA_Income_base_firsts_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_firsts, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_base_percentages_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_percentages, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_base_first_race_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(base_first_race_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c6c060b8-a88a-4834-9fdc-98a7c9f87f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_first_race_shap_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f221e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_ranks = list()\n",
    "# base_avg_shaps = list()\n",
    "# base_top_count = list()\n",
    "# base_top_avg_shaps = list()\n",
    "# for sv in base_result['shap_vals']:\n",
    "#     sv = sv.values\n",
    "#     base_rank= analyze_shap(sv)\n",
    "#     base_ranks.append(base_rank)\n",
    "#     age_rank = base_rank[:,-1]\n",
    "#     base_first = [int(j) for j in range(len(age_rank)) if int(age_rank[j]) == 1]\n",
    "#     base_first_count = len(base_first)\n",
    "#     sv = sum_race_shaps(sv)\n",
    "#     base_avg_shaps.append(np.mean(np.abs(sv[:,-1])))\n",
    "    \n",
    "#     base_top_avg_shaps.append(np.mean(np.abs(sv[base_first,-1])))\n",
    "#     base_top_count.append(base_first_count)\n",
    "\n",
    "# base_avg_shaps_m = np.mean(base_avg_shaps)\n",
    "# base_top_count_m = np.mean(base_top_count)\n",
    "# base_top_avg_shaps_m = np.mean(base_top_avg_shaps)\n",
    "\n",
    "# base_avg_shaps_s = np.std(base_avg_shaps)\n",
    "# base_top_count_s = np.std(base_top_count)\n",
    "# base_top_avg_shaps_s = np.std(base_top_avg_shaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c3611",
   "metadata": {},
   "source": [
    " # Race Bucetkization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1a2f676a-6a05-4cbf-812f-fbd64b53d34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "**********START**********\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    }
   ],
   "source": [
    "bucket_fids = list()\n",
    "bucket_shap_vals = list()\n",
    "bucket_race_ranks = list()\n",
    "bucket_firsts = list()\n",
    "bucket_percentages = list()\n",
    "bucket_first_race_shap_vals = list()\n",
    "\n",
    "\n",
    "for race in all_races:\n",
    "    # compute bin edges as shap values for bucketized data\n",
    "    b_fids = list()\n",
    "    b_shap_vals = list()\n",
    "    b_race_ranks = list()\n",
    "    b_firsts = list()\n",
    "    b_percentages = list()\n",
    "    \n",
    "    b_first_race_shap_vals = list()\n",
    "\n",
    "\n",
    "    X2 = assign_race(X, race)\n",
    "    for i, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
    "        for train_val_idx, test_idx in splitter.split(X2, Y):\n",
    "            X_train_val, X_test = X2.iloc[train_val_idx], X2.iloc[test_idx]\n",
    "            Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=seed)\n",
    "        for train_idx, val_idx in splitter.split(X_train_val, Y_train_val):\n",
    "            X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
    "            Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx] \n",
    "\n",
    "        b_shap = compute_shap(X_train,X_test,Y_train,Y_test,seed)\n",
    "        sv = sum_race_shaps(b_shap.values)\n",
    "        race_sv = sv[:,-1]\n",
    "        b_rank, b_race_rank = get_ranks(sv)\n",
    "\n",
    "        preds = base_preds[i]\n",
    "        base = b_shap.base_values\n",
    "        b_fid, b_agreed = compute_fidelity(preds,sv,base)\n",
    "\n",
    "\n",
    "        b_fids.append(b_fid)\n",
    "        b_race_ranks.append(b_race_rank)\n",
    "        b_shap_vals.append(race_sv)\n",
    "        \n",
    "        # First\n",
    "\n",
    "        first = [int(j) for j,v in enumerate(b_race_rank) if v == 1]\n",
    "        \n",
    "        b_firsts.append(first)\n",
    "        b_percentages.append(len(first)/len(X_test)*100 )\n",
    "        b_first_race_shap_vals.append(race_sv[first])\n",
    "    \n",
    "    bucket_fids.append(b_fids)\n",
    "    bucket_shap_vals.append(b_shap_vals)\n",
    "    bucket_race_ranks.append(b_race_ranks)\n",
    "    bucket_firsts.append(b_firsts)\n",
    "    bucket_percentages.append(b_percentages)\n",
    "    bucket_first_race_shap_vals.append(b_first_race_shap_vals)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8ed942ff-4915-4d53-9f0f-53afb78f6104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([-0.77549565, -0.85126317]),\n",
       " array([], dtype=float64)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_first_race_shap_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "664954a0-36fc-4579-aadb-a7c2befa851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + '/Bucket_OVA_Income_bucket_fids_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(bucket_fids, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_bucket_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(bucket_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_bucket_race_ranks_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(bucket_race_ranks, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_bucket_firsts_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(bucket_firsts, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_bucket_percentages_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(bucket_percentages, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path + '/Bucket_OVA_Income_bucket_first_race_shap_vals_cv.pickle', 'wb') as f:\n",
    "    pickle.dump(bucket_first_race_shap_vals, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "750f99b8-c058-4ab0-b19b-c4e6a7e38ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ew_bucket_downs = list()\n",
    "# ew_bucket_avg_ranks = list()\n",
    "# ew_bucket_top_avg_ranks = list()\n",
    "# ew_bucket_top_counts = list()\n",
    "\n",
    "\n",
    "# ew_bucket_fids = list()\n",
    "\n",
    "# for race in all_races:\n",
    "#     # compute bin edges as shap values for bucketized data\n",
    "\n",
    "#     cut5_result = compute_shap(X,Y,base_result['models'],race)\n",
    "#     cut5_ranks = list()\n",
    "#     race_ranks = list()\n",
    "#     for i,sv in enumerate(cut5_result['shap_vals']):\n",
    "#         sv = sv.values\n",
    "#         cut5_rank = analyze_shap(sv,plot=False)\n",
    "#         cut5_ranks.append(cut5_rank)\n",
    "#         race_rank = np.mean(cut5_rank[:,-1])\n",
    "#         race_ranks.append(race_rank)\n",
    "#     # Compute fidelity and indices where explanation is the same.\n",
    "#     cut5_fidelities = list()\n",
    "#     cut5_agreeds = list()\n",
    "#     for i in range(FOLDS):\n",
    "#         preds = base_result['preds'][i]\n",
    "#         sv = cut5_result['shap_vals'][i].values\n",
    "#         base = cut5_result['shap_vals'][i].base_values\n",
    "#         cut5_fidelity,cut5_agreed = compute_fidelity(preds,sv,base)\n",
    "#         cut5_fidelities.append(cut5_fidelity)\n",
    "#         cut5_agreeds.append(cut5_agreed)\n",
    "#     avg_fid = np.mean(cut5_fidelities)\n",
    "#     avg_rank = np.mean(race_ranks)\n",
    "\n",
    "\n",
    "#     # Compare shap values, rank differences and kendalls\n",
    "#     shap_difs = list()\n",
    "#     rank_difs = list()\n",
    "#     kendalls = list()\n",
    "#     avg_shaps = list()\n",
    "#     for i in range(FOLDS):\n",
    "#         s1 = sum_race_shaps(base_result['shap_vals'][i])\n",
    "#         s2 = sum_race_shaps(cut5_result['shap_vals'][i])\n",
    "#         _,_,rank_dif,_ = compare_results(s1, s2, base_ranks[i], race_ranks[i] ,cut5_agreeds[i])\n",
    "#         rank_difs.append(rank_dif)\n",
    "        \n",
    "\n",
    "    \n",
    "#     ew_bucket_fids.append(avg_fid)\n",
    "#     ew_bucket_avg_ranks.append(avg_rank)\n",
    "\n",
    "\n",
    "#     first_rank_shifts,base_first = get_first_rank_shift(base_ranks, rank_difs)\n",
    "#     first_down = down_percent(first_rank_shifts)\n",
    "\n",
    "#     ew_top_counts.append(np.mean([len(b) for b in base_first]))\n",
    "\n",
    "#     # Get top shap vals\n",
    "#     top_avg_shaps = list()\n",
    "#     for i in range(FOLDS):\n",
    "#         sv = sum_race_shaps(cut5_result['shap_vals'][i].values)\n",
    "        \n",
    "#         first_sv = abs(sv[base_first[i]])\n",
    "#         cut5_avg_shap = np.mean(sv[:,-1])\n",
    "#         top_avg_shaps.append(np.mean(first_sv))\n",
    "#     ed_top_avg_shaps.append(top_avg_shaps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "14bd7fd4-2c92-4c1b-ae7a-3adc11ba6674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base rank 7.46\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ew_bucket_avg_ranks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m base_rank = np.mean(base_ranks[\u001b[32m0\u001b[39m][:,-\u001b[32m1\u001b[39m])\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbase rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_rank\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i,v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mew_bucket_avg_ranks\u001b[49m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v:\n\u001b[32m      8\u001b[39m         cur_avg_rank = v\n",
      "\u001b[31mNameError\u001b[39m: name 'ew_bucket_avg_ranks' is not defined"
     ]
    }
   ],
   "source": [
    "cut5_avg_ranks = list()\n",
    "attack_avg_ranks = list()\n",
    "attack_avg_fidelities = list()\n",
    "base_rank = np.mean(base_ranks[0][:,-1])\n",
    "print(f'base rank {base_rank:.2f}')\n",
    "for i,v in enumerate(ew_bucket_avg_ranks):\n",
    "    if v:\n",
    "        cur_avg_rank = v\n",
    "        cur_fidelity = float(ew_bucket_fids[i])\n",
    "        \n",
    "\n",
    "        print(f'Attack rank: {cur_avg_rank:.2f}  new fid: {cur_fidelity:.4f}')\n",
    "\n",
    "    \n",
    "    attack_avg_ranks.append(cur_avg_rank)\n",
    "    attack_avg_fidelities.append(cur_fidelity)\n",
    "attack_avg_ranks = [abs(r) for r in attack_avg_ranks if r != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bcacb-f84d-46e6-8d70-22552f9c9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_bucket_avg_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e18801-e62d-4fea-960c-a2e8705c2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlresearch\n",
    "mlresearch.utils.set_matplotlib_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82078e63-0554-4bc1-afd0-2bf1a118530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Race_Income_base_rank_OVA.pickle', 'wb') as f:\n",
    "    pickle.dump(base_rank, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('Race_Income_avg_ranks_OVA.pickle', 'wb') as f:\n",
    "    pickle.dump(attack_avg_ranks, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca748cb6-2de1-4c89-8d4f-c9336ccef6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = ['Base']+ all_races\n",
    "# Bar width and positions\n",
    "bar_width = 0.4\n",
    "x_indices = np.arange(len(x_values))\n",
    "\n",
    "# Plot bars\n",
    "plt.bar(x_indices,[base_rank] + attack_avg_ranks, width=bar_width, color='blue')\n",
    "\n",
    "# Labeling and styling\n",
    "plt.xticks(ticks=x_indices, labels=x_values)\n",
    "plt.xlabel('Race Buckets')\n",
    "plt.ylabel('Average Ranks')\n",
    "plt.title('ACS Income Ranks')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabcdc7-6ff7-4325-b313-cfd5b799f958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5fec7-b9dc-4a6f-872d-31616a132e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793fac23-6a7a-4d0c-843d-74befbad0c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
